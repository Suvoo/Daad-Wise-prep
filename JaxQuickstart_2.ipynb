{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JaxQuickstart-2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfcAkP+qcgaEVEyB53Xfu9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suvoo/Daad-Wise-prep/blob/main/JaxQuickstart_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The goal of this notebook will be to gain the knowledge necessary to build complex ML models (such as NNs) and train them in parallel on multiple devices! 💻💻💻"
      ],
      "metadata": {
        "id": "6VSwmWGziLll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mkDlETimg_sZ"
      },
      "outputs": [],
      "source": [
        "# Let's import the necessary packages\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "from jax import grad, jit, vmap, pmap #parallelize-->pmap\n",
        "\n",
        "from jax import random\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from typing import Tuple, NamedTuple\n",
        "import functools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Problem of State\n",
        "JAX ❤️ Pure Functions => JAX \"!❤️\" State."
      ],
      "metadata": {
        "id": "Loy_yNARdPXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) We've seen in the last notebook/video that impure functions are problematic.\n",
        "\n",
        "g = 0.  # state\n",
        "\n",
        "# We're accessing some external state in this function which causes problems\n",
        "def impure_uses_globals(x):\n",
        "    return x + g\n",
        "\n",
        "# JAX captures the value of the global/state during the first run\n",
        "print (\"First call: \", jit(impure_uses_globals)(4.))\n",
        "\n",
        "# Let's update the global/state!\n",
        "g = 10.\n",
        "\n",
        "# Subsequent runs may silently use the cached value of the globals/state\n",
        "print (\"Second call: \", jit(impure_uses_globals)(5.))"
      ],
      "metadata": {
        "id": "MlqNWYhQiNX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70232d3-7184-4fec-d328-0e44262514e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First call:  4.0\n",
            "Second call:  5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) We've also seen this pattern how JAX's PRNG \n",
        "# (which is not stateful in contrast to NumPy's PRNG) is handling state.\n",
        "\n",
        "seed = 0\n",
        "state = jax.random.PRNGKey(seed)\n",
        "\n",
        "# We input the state, we somehow manipulate it and we return it back.\n",
        "# The state is not saved internally.\n",
        "state1, state2 = jax.random.split(state)  # recall: key/subkey was the terminology we used"
      ],
      "metadata": {
        "id": "Y8wTN4upeN-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now explictly address and understand the problem of state!\n",
        "# Why? \n",
        "# Well, NNs love statefulness: model params, optimizer params, BatchNorm, etc.\n",
        "# and we've seen that JAX seems to have a problem with it.\n",
        "\n",
        "class Counter:\n",
        "    \"\"\"A simple counter.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.n = 0\n",
        "    def count(self) -> int:\n",
        "        \"\"\"Increments the counter and returns the new value.\"\"\"\n",
        "        self.n += 1\n",
        "        return self.n\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the counter to zero.\"\"\"\n",
        "        self.n = 0\n",
        "    \n",
        "counter =  Counter()\n",
        "\n",
        "for _ in range(3): # works fine\n",
        "    print(counter.count())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7gyMNCAebUh",
        "outputId": "dbfabc00-a40e-45cf-b97b-760cf6860cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter.reset()\n",
        "fast_count = jit(counter.count)\n",
        "\n",
        "for _ in range(3): # not working\n",
        "    print(fast_count()) # as count is not pure, so jit fails...cache \"1\" from first time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdNYbBJqfQe9",
        "outputId": "f0a16412-f540-485e-e2a5-c13f995323fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import make_jaxpr # use jaxpr to understand why this is happening\n",
        "\n",
        "counter.reset()\n",
        "print(make_jaxpr(counter.count)()) # return 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESj8_MF2fluT",
        "outputId": "f4b8d483-9f00-4c23-9b9e-4223d187038b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ lambda ; . let  in (1,) }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter.reset()\n",
        "counter.count() # --> modifies state to 1\n",
        "fast_count = jit(counter.count)\n",
        "\n",
        "for _ in range(3): # not working\n",
        "    print(fast_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC8O_ciQgQ_1",
        "outputId": "50632354-b197-405f-adc4-576c2c720df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import make_jaxpr # use jaxpr to understand why this is happening\n",
        "\n",
        "counter.reset()\n",
        "counter.count()\n",
        "print(make_jaxpr(counter.count)()) # return 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRYHSckhgpBC",
        "outputId": "c261ce15-be29-4d8f-f3ce-228553359568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ lambda ; . let  in (2,) }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Solution to avoid creating impure function"
      ],
      "metadata": {
        "id": "BguV0l2fg3GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CounterState =  int # implemented as a simple integer\n",
        "\n",
        "class CounterV2:\n",
        "    def count(self, n: CounterState) -> Tuple[int, CounterState]:\n",
        "        # You could just return n+1, but here we separate its role as \n",
        "        # the output and as the counter state for didactic purposes.\n",
        "        # (as the output may be some arbitrary function of state in general case)\n",
        "        return n+1,n+1\n",
        "\n",
        "    def reset(self) -> CounterState:\n",
        "        return 0\n",
        "\n",
        "counter = CounterV2()\n",
        "state = counter.reset() # notice how reset() now returns state (external vs internal imp)\n",
        "\n",
        "for _ in range(3):\n",
        "    value,state =  counter.count(state)\n",
        "    print(value,state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rILYmCDegxIQ",
        "outputId": "fde9c667-18d7-49be-e2bc-29f5fbbc964e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1\n",
            "2 2\n",
            "3 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = counter.reset()\n",
        "fast_count = jit(counter.count)\n",
        "\n",
        "for _ in range(3):\n",
        "    value,state =  fast_count(state)\n",
        "    print(value,state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WXVN0C-jIfL",
        "outputId": "7b6efd08-ddeb-4bdb-ff31-2020b87a7b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1\n",
            "2 2\n",
            "3 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import make_jaxpr # use jaxpr to understand why this is happening\n",
        "\n",
        "counter.reset()\n",
        "print(make_jaxpr(counter.count)(10)) # "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq9l5yDVjRkA",
        "outputId": "d98bba72-c407-4172-d704-c42cca439505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{ lambda ; a:i32[]. let b:i32[] = add a 1; c:i32[] = add a 1 in (b, c) }\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary we used the following rule to convert a stateful class:\n",
        "\n",
        "```python\n",
        "class StatefulClass\n",
        "\n",
        "    state: State\n",
        "\n",
        "    def stateful_method(*args, **kwargs) -> Output:\n",
        "```\n",
        "\n",
        "into a class of the form:\n",
        "\n",
        "```python\n",
        "class StatelessClass\n",
        "\n",
        "    def stateless_method(state: State, *args, **kwargs) -> (Output, State):\n",
        "```"
      ],
      "metadata": {
        "id": "7bLwMwkrkNDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice - we figured an equivalent way to handle states without introducing the side-effects.\n",
        "\n",
        "This brings us 1 step closer to building neural networks! 🥳\n",
        "\n",
        "We still need to find a way to handle gradients when dealing with big NNs."
      ],
      "metadata": {
        "id": "Z8KcTOrTkR68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enter PyTree 🌳\n",
        "Before we start - why are gradients a problem in the first place?"
      ],
      "metadata": {
        "id": "asNfk47prZuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x, y, z, w: x**2 + y**2 + z**2 + w**2  # + 175B params more (minus 4)\n",
        "\n",
        "# JAX: .backward() is not that great\n",
        "# also JAX:\n",
        "x, y, z, w = [1.]*4\n",
        "dfdx, dfdy, dfdz, dfdw = grad(f, argnums=(0, 1, 2, 3))(x, y, z, w) # these have a lot of paarameters and donot scale\n",
        "print(dfdx, dfdy, dfdz, dfdw)\n",
        "\n",
        "# Great now we just need to update our params!\n",
        "# lr = 0.001\n",
        "# x -= lr*dfdx\n",
        "# y -= lr*dfdy\n",
        "# ... (175B lines later)\n",
        "# w -= lr*dfdw\n",
        "\n",
        "# No, no, no. \n",
        "# We do have a better way."
      ],
      "metadata": {
        "id": "WVlM2enEjae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f271c920-7e3f-4cda-9af4-2dfb2d83d88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0 2.0 2.0 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, no, no. We do have a better way!\n",
        "\n",
        "We want to, more naturally, wrap our params in some more complex data structures like dictionaries, etc.\n",
        "\n",
        "JAX knows how to deal with these! The answer is called a PyTree."
      ],
      "metadata": {
        "id": "6dv3hhOHrlSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # A contrived example for pedagogical purposes\n",
        "# (if your mind needs to attach some semantics to parse this - treat it as model params)\n",
        "pytree_example = [\n",
        "    [1, 'a', object()], # pytree 1 with 3 leaves\n",
        "    (1, (2, 3), ()),\n",
        "    [1, {'k1': 2, 'k2': (3, 4)}, 5],\n",
        "    {'a': 2, 'b': (2, 3)},\n",
        "    jnp.array([1, 2, 3]),\n",
        "]\n",
        "\n",
        "# Let's see how many leaves they have:\n",
        "for pytree in pytree_example:\n",
        "    leaves = jax.tree_leaves(pytree)  # handy little function\n",
        "    print(f\"{repr(pytree):<45} has {len(leaves)} leaves: {leaves}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_pn4h-KrmB1",
        "outputId": "2762f95b-8207-4594-a5eb-a0b12405edea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 'a', <object object at 0x7fc99f412770>]   has 3 leaves: [1, 'a', <object object at 0x7fc99f412770>]\n",
            "(1, (2, 3), ())                               has 3 leaves: [1, 2, 3]\n",
            "[1, {'k1': 2, 'k2': (3, 4)}, 5]               has 5 leaves: [1, 2, 3, 4, 5]\n",
            "{'a': 2, 'b': (2, 3)}                         has 3 leaves: [2, 2, 3]\n",
            "DeviceArray([1, 2, 3], dtype=int32)           has 1 leaves: [DeviceArray([1, 2, 3], dtype=int32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we manipulate PyTrees?\n",
        "\n",
        "list_of_lists = [\n",
        "    {'a': 3},\n",
        "    [1, 2, 3],\n",
        "    [1, 2],\n",
        "    [1, 2, 3, 4]\n",
        "]\n",
        "\n",
        "# For single arg functions use tree_map\n",
        "# tree_map iterates through leaves and applies the lambda function\n",
        "print(jax.tree_map(lambda x: x*2, list_of_lists))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDTduQMqsvtt",
        "outputId": "39e89c0d-1a18-47b9-ac3e-17a0c44536ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'a': 6}, [2, 4, 6], [2, 4], [2, 4, 6, 8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "another_list_of_lists = list_of_lists\n",
        "print(jax.tree_multimap(lambda x, y: x+y, list_of_lists, another_list_of_lists))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0RXolCYs7bv",
        "outputId": "353844d2-8565-4b2a-c50d-79a145ac01e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'a': 6}, [2, 4, 6], [2, 4], [2, 4, 6, 8]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py:189: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
            "  'instead as a drop-in replacement.', FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTrees need to have the same structure if we are to apply tree_multimap!\n",
        "another_list_of_lists = deepcopy(list_of_lists)\n",
        "another_list_of_lists.append([23]) # change not possinle\n",
        "print(jax.tree_multimap(lambda x, y: x+y, list_of_lists, another_list_of_lists))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "F248M40YtBs5",
        "outputId": "168c2ba1-882d-40e6-86c2-ddebdc32c7cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py:189: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
            "  'instead as a drop-in replacement.', FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-81259d1bd20e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0manother_list_of_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0manother_list_of_lists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_multimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manother_list_of_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36mtree_multimap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m   warnings.warn('jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() '\n\u001b[1;32m    189\u001b[0m                 'instead as a drop-in replacement.', FutureWarning)\n\u001b[0;32m--> 190\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;31m# TODO(mattjj,phawkins): consider removing this function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \"\"\"\n\u001b[1;32m    182\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \"\"\"\n\u001b[1;32m    182\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: List arity mismatch: 5 != 4; list: [{'a': 3}, [1, 2, 3], [1, 2], [1, 2, 3, 4], [23]]."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Less contrived example: training a toy MLP (multi-layer perceptron) model"
      ],
      "metadata": {
        "id": "LAPvN3yztT6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_mlp_params(layer_widths):\n",
        "    params = []\n",
        "\n",
        "    # Allocate weights and biases (model parameters)\n",
        "    # Notice: we're not using JAX's PRNG here - doesn't matter for this simple example\n",
        "    for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n",
        "        params.append(\n",
        "            dict(weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2/n_in),\n",
        "                biases=np.ones(shape=(n_out,))\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return params\n",
        "\n",
        "# Instantiate a single input - single output, 3 layer (2 hidden layers) deep MLP\n",
        "params = init_mlp_params([1, 128, 128, 1])\n",
        "\n",
        "# Another example of how we might use tree_map - verify that shapes make sense:\n",
        "jax.tree_map(lambda x: x.shape, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAlNNTrdtVu8",
        "outputId": "9988ce68-0395-4f04-f5d7-028d102650fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'biases': (128,), 'weights': (1, 128)},\n",
              " {'biases': (128,), 'weights': (128, 128)},\n",
              " {'biases': (1,), 'weights': (128, 1)}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(params, x):\n",
        "    *hidden, last = params\n",
        "\n",
        "    for layer in hidden:\n",
        "        x = jax.nn.relu(jnp.dot(x, layer['weights']) + layer['biases'])\n",
        "\n",
        "    return jnp.dot(x, last['weights']) + last['biases']\n",
        "\n",
        "def loss_fn(params, x, y):\n",
        "    return jnp.mean((forward(params, x) - y) ** 2)  # MSE loss\n",
        "\n",
        "lr = 0.0001\n",
        "\n",
        "@jit  # notice how we do jit only at the highest level - XLA will have plenty of space to optimize\n",
        "def update(params, x, y):\n",
        "\n",
        "    # Note that grads is a pytree with the same structure as params.\n",
        "    # grad is one of the many JAX functions that has built-in support for pytrees!\n",
        "    grads = jax.grad(loss_fn)(params, x, y)\n",
        "\n",
        "    # Task: analyze grads and make sure it has the same structure as params\n",
        "\n",
        "    # SGD update\n",
        "    return jax.tree_multimap(\n",
        "        lambda p, g: p - lr * g, params, grads  # for every leaf i.e. for every param of MLP\n",
        "    )"
      ],
      "metadata": {
        "id": "HbV2p-yaxnse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = np.random.normal(size=(128, 1))\n",
        "ys = xs ** 3  # let's learn how to regress a parabola\n",
        "\n",
        "# Task experiment a bit with other functions (polynomials, sin, etc.)\n",
        "\n",
        "num_epochs = 5000\n",
        "for _ in range(num_epochs):\n",
        "    params = update(params, xs, ys)  # again our lovely pattern\n",
        "\n",
        "plt.scatter(xs, ys)\n",
        "plt.scatter(xs, forward(params, xs), label='Model prediction')\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "1QKxN_AtxwkH",
        "outputId": "8c8ba16c-231c-4357-cb16-7e208fd15651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfXhU5Z3/8fd3JpMwgBIFFEmguHtZWoEIdYrW4sOKitWKwK9FqbVW6mJ/faC6La6sXiLUKpXdau3a1tTaupcPQBFSrOuyQte6aW0lMRhAyk9raUnQGsDQIgNJJvfvj5kkkzCTp3nMyed1XWlm7nNyzj2mfDh8z33u25xziIiIN/ly3QEREckchbyIiIcp5EVEPEwhLyLiYQp5EREPK8h1B+KNGjXKTZgwIdfdEBEZUKqrq/c750Yn2pZXIT9hwgSqqqpy3Q0RkQHFzP6UbJvKNSIiHqaQFxHxMIW8iIiH5VVNPpHm5mbq6uo4evRorrsiaTZkyBBKS0sJBAK57oqIZ+V9yNfV1XHCCScwYcIEzCzX3ZE0cc5x4MAB6urqOP3003PdHRHPyvtyzdGjRxk5cqQC3mPMjJEjR+pfaCK1a+GByXB3cfR77dq0Hj7vr+QBBbxH6fcqg17tWnh2MTSHo+8P7Y2+Byibn5ZTpHwlb2bjzOx/zOx1M9tpZl+LtZ9sZi+Y2Rux7yel3l0REQ/ZsqIj4Ns0h6PtaZKOck0L8HXn3JnAucCXzexM4HZgi3PuDGBL7P2AZGZ89rOfbX/f0tLC6NGj+eQnP9mn40yYMIH9+/envE+q9uzZw+TJkwGoqqpi8eLF3e5/7733dnp/3nnnZaxvIoPKobq+tfdDyiHvnHvbOfdq7PXfgF1ACXA18Hhst8eBOameK1eGDRvGjh07CIejf+O+8MILlJSU5LhXx2tpaenzz4RCIR566KFu9+ka8r/5zW/6fB4ROd6R4Jg+tfdHWm+8mtkEYBrwO+BU59zbsU3vAKem81xJZegmxhVXXMFzzz0HwNNPP82CBQvatx08eJA5c+ZQVlbGueeeS21tLQAHDhzgsssuY9KkSdx0003Er8L1xBNPMH36dKZOncrNN99MJBLp9vzDhw/n1ltvZdKkScycOZOGhgYALrroIm655RZCoRDf/e53qa6u5sILL+Tss89m1qxZvP129FdQXV3NWWedxVlnncXDDz/cftwXX3yx/V8khw8f5sYbb2TKlCmUlZXxzDPPcPvttxMOh5k6dSrXXXdde18gOkJmyZIlTJ48mSlTprBmzZr2Y1500UV86lOf4kMf+hDXXXcdWoFM5Hj3N1/DEVfYqe2IK+T+5mvSdo60hbyZDQeeAW5xzv01fpuL/glP+KfczBaZWZWZVbUFV7+13cQ4tDd6urabGGkI+muvvZbVq1dz9OhRamtrOeecc9q3LVu2jGnTplFbW8u9997L5z73OQCWL1/OjBkz2LlzJ3PnzuXPf/4zALt27WLNmjX8+te/Ztu2bfj9fp588sluz//+++8TCoXYuXMnF154IcuXL2/f1tTU1F52+epXv8q6deuorq5m4cKF3HHHHQDceOONfO973+O1115Leo5vfvObjBgxgu3bt1NbW8vFF1/MypUrCQaDbNu27bg+rl+/nm3btvHaa6+xefNmlixZ0v6XSk1NDQ8++CCvv/46b731Fr/+9a/78F9bZHB4/PB0bm++ibrWUbQ6o651FLc338Tjh6en7RxpGV1jZgGiAf+kc259rPkvZnaac+5tMzsNeDfRzzrnyoFygFAolNrlXnc3MVK8U11WVsaePXt4+umnueKKKzptq6ys5JlnngHg4osv5sCBA/z1r3/lpZdeYv366H+OK6+8kpNOit573rJlC9XV1Xz0ox8FIBwOc8opp3R7fp/PxzXXRP92/+xnP8u8efPat7W17969mx07dnDppZcCEIlEOO2002hsbKSxsZELLrgAgOuvv57nn3/+uHNs3ryZ1atXt79v628ylZWVLFiwAL/fz6mnnsqFF17I1q1bOfHEE5k+fTqlpaUATJ06lT179jBjxoxujycy2IwtDrKxcQYbmzr/2SgpDqbtHCmHvEXHwf0Y2OWc+07cpo3ADcDK2Pefp3quHmX4Jsbs2bP5xje+wYsvvsiBAwf6fRznHDfccAP33Xdfv48RP/xw2LBh7cedNGkSL7/8cqd9Gxsb+32e/ioqKmp/7ff7+3W/QMTrlsyayNL12wk3d5RrgwE/S2ZNTNs50lGu+ThwPXCxmW2LfV1BNNwvNbM3gEti7zNrRGnf2vto4cKFLFu2jClTpnRqP//889tLGS+++CKjRo3ixBNP5IILLuCpp54C4Pnnn+e9994DYObMmaxbt453343+4+bgwYP86U9JZwoFoLW1lXXr1gHw1FNPJbwqnjhxIg0NDe0h39zczM6dOykuLqa4uJjKykqApKWhSy+9tFO9vq2/gUCA5ubm4/Y///zzWbNmDZFIhIaGBl566SWmT0/fPzNFvG7OtBLumzeFkuIgRvQK/r55U5gzLX0DO1K+knfOVQLJnmqZmerx+2TmXZ0fLAAIBKPtaVBaWppwuOHdd9/NwoULKSsrY+jQoTz+eHRQ0bJly1iwYAGTJk3ivPPOY/z48QCceeaZ3HPPPVx22WW0trYSCAR4+OGH+cAHPpD03MOGDeOVV17hnnvu4ZRTTmm/yRmvsLCQdevWsXjxYg4dOkRLSwu33HILkyZN4ic/+QkLFy7EzLjssssSnuPOO+/ky1/+MpMnT8bv97Ns2TLmzZvHokWLKCsr4yMf+UinvyDmzp3Lyy+/zFlnnYWZcf/99zNmzBh+//vf9+m/q8hgNmdaSVpDvSvLp1EPoVDIdV00ZNeuXXz4wx/u/UFq10Zr8IfqolfwM+9K25NjuTR8+HAOHz6c626kXZ9/vyJyHDOrds6FEm0bENMa9EnZfE+EuohIOuT9BGUS5cWreBHJvAER8vlUUpL00e9VJPPyPuSHDBnCgQMHFAge0zaf/JAhQ3LdFRFPy/uafGlpKXV1daT8NKzknbaVoUQkc/I+5AOBgFYOEhHpp7wv14iISP8p5EVEPEwhLyLiYXlfkxcRGYgqaupZtWk3+xrDjC0OsmTWxIxOX5CMQl5EJM0qauqp3PB91rCasUX72XdkFA9uuBb4UtaDXuUaEZE02/ZcOSusnFLffnwGpb79rLBytj1XnvW+KORFRNLspqYnGGpNndqGWhM3NT2R9b4o5EVE0mysL/GiQsnaM0khLyKSZkeDY/rUnkkKeRGRNKmoqefjK3/J0kNzCVPYaVuLfwhDP7Ei631KS8ib2WNm9q6Z7Yhru9vM6rssCSgi4kntI2qO/CMPBL7PkdZCDroTcBiMGEfB1d/LyVoX6bqS/ylweYL2B5xzU2Nf/5mmc4mI5JWKmnpeXPdwpxE1I32HGcIxlhd8DW7dkbPFjNIS8s65l4CD6TiWiMhAUlFTz9L12/mGf03ejKiJl+ma/FfMrDZWzjkp0Q5mtsjMqsysStMJi8hAs2rTbi6N/IoS259wey5G1MTLZMj/APh7YCrwNvBviXZyzpU750LOudDo0aMz2B0RkfQL/fUFVgYexSzx9lyMqImXsZB3zv3FORdxzrUCPwKmZ+pcIiK5srTwZ8eVadrkakRNvIyFvJmdFvd2LrAj2b4iIgPVqSQu0zjI2YiaeGmZoMzMngYuAkaZWR2wDLjIzKYS/ax7gJvTcS4RkXxiI0rh0N4E7eNyHvCQppB3zi1I0PzjdBxbRCSvzbwLnl0MzeGOtkAw2p4H9MSriEgqyubDVQ/BiHEQe/CJqx7Ki6t40HzyIiKpK5ufN6Hela7kRUQ8TCEvIuJhCnkREQ9TyIuIeJhCXkTEwxTyIiIeppAXEfEwhbyIiIcp5EVEPEwhLyLiYQp5EREPU8iLiHiYQl5ExMMU8iIiHpaWkDezx8zsXTPbEdd2spm9YGZvxL6flI5ziYhI76XrSv6nwOVd2m4HtjjnzgC2xN6LiEgWpSXknXMvAQe7NF8NPB57/TgwJx3nEhFJSe1aeGAy3F0c/V67Ntc9yqhMrgx1qnPu7djrd4BTE+1kZouARQDjx4/PYHdEZNCrXdt5PdZDe6PvIW9XdkpVVm68Oucc4JJsK3fOhZxzodGjR2ejOyIyWG1Z0XnBbYi+37IiN/3JgkyG/F/M7DSA2Pd3M3guEZGeHarrW7sHZDLkNwI3xF7fAPw8g+cSEUksrgbfapZwlyPBMVnuVPakawjl08DLwEQzqzOzLwArgUvN7A3gkth7EZHsaavBH9oLOHyuFdelcHzEFXJ/8zU56V42pOXGq3NuQZJNM9NxfBGRfklQgzeDFufDh2OfG8n9LfN59th07s5NDzMuk6NrRERyK0mt3Yfj74492f6+pDiYrR5lnaY1EBHPSlZr3+dGtr8OBvwsmTUxW13KOoW8iHjW/c3XcMQVdmo74gr518g1GNEr+PvmTWHOtJLcdDALVK4REc96/PB0DvqauK1gLWPtQEcNvvXj/HHllbnuXlYo5EXEs8YWB9nYOIONTTM6tXu5Bt+VyjUi4llLZk0kGPB3avN6Db4rXcmLiGe11dpXbdrNvsYwY4uDLJk10dM1+K4U8iLiaXOmlQyqUO9K5RoREQ9TyIuIeJhCXkTEwxTyIiIeppAXEfEwhbyIiIcp5EVEPEwhLyLiYRl/GMrM9gB/AyJAi3MulOlziog3VNTUD+qnVdMhW0+8/oNzbn+WziUiHlBRU0/lhu+zhtWMLdrPviOjeHDDtcCXFPR9oHKNiOSlbc+Vs8LKKfXtx2dQ6tvPCitn23Plue7agJKNK3kH/LeZOeAR51yn35CZLQIWAYwfPz4L3RGRfBVfnvnfwicY6mvqtH2oNXFT0xPA8tx0cADKRsjPcM7Vm9kpwAtm9nvn3EttG2OhXw4QCoVcsoOIiLdV1NSzdP12ws0RAMZa4grvWN+BbHZrwMt4ucY5Vx/7/i6wAZie6XOKyMCzatPu9oAH2OdGJdzvaJJ1WyWxjIa8mQ0zsxPaXgOXATsyeU4RGZj2NYaZ7auksnAxbxV9hiBHaXKdiw0t/iEM/cSKHPVwYMp0ueZUYIOZtZ3rKefcf2X4nCIyANxZsZ2nf7eXiHP4zfg/hb9hhT3KUIvW4UfaYY45P42cQDGHYUQpBTPvgrL5Oe75wJLRkHfOvQWclclziMjAc2fFdp747Z/b30ec4xZWtwd8myKLEAkOh3+uy3YXPUMrQ4lIVlTU1HP3xp00hpsBWF7wGNf5f4mfViL48NGa8OeGht/JZjc9RyEvIhlVUVPPv6yv5UhzK7N9ldxWuJaxth8DopVcKKAVl2xs3YjSbHXVkxTyIpIxFTX1/OqZh/lf3+OcXHQY6Aj2rszAuS7bA0GYeVfmO+pheuJVRDJm23PlrPT/kJG+w5glD/h2BowYF30xYhxc9ZButKZIV/IikjE3NT1BkS/S844xZn64VaOs00khLyL91tMskX1+OvXsz6e3g6KQF5H+ubNiO3975SnWFKxNOkvk0eAYhobfTnoMR7RCg/mjAf/J72Sh54OLQl5E+mTrxkcoefV+vun2Q6Cjzl5q+1nhyrn/uQLmTItOIDb0EyuIbPi/+F1LwmNZ6AsK9gzTjVcR6bWtGx9hcvWdjGV/whupHbNExpTNxz/3BxA8ufOOwZNh3o8U8FmgK3kRSWjrxkcY9+oqTnENvGuj2fuRJYx7dRXBLk+ldnVcHb5svkbI5JBCXkSAzqF+yE6gzB2hyCJgMIYGRlTfyRCaYkX05I4GxzA0O12WXlDIiwxGtWthywo4VAcjSvlD8ceZvGd99Crd4CT+dlyYB62JFuejIMn0AxC9kapZIvOLQl7Ey7qEedvToy0//yoFkaPRfQ7t5fRDq/H19KAS4KOVsCtMWLJxxG6kqjSTVxTyIl5RuxaevQWa3481GK2Aj9ikMIf20vLzrxLxFVHUFvAxvR2BEV+bP9U1EDEfflqxEeMwTQOclxTyIgNR7Vp4/p8hfBBH7Cq667wvuOPCuyByFH+XgO+tsCtk79lL+Ojsm2H2zdHj9etIkk36Hcng0F622Eu02JzqcsI9HCMwDAqKIPweBE+ClmNxV9hdD+UD17nO7dr/p0MLfgqItG9vK68YbQ8U9bLr7U8gddbq6FSyaXIFvG9DGOHe510b1RHwMqBkPOTN7HLgu4AfeNQ5tzLT5xSP+cU/QfVPwUWigVgQjAam+aNtgWHQEo4GZZcnJ7dufIQzXv0mI9zf4q5y07FefA/HaH6/I9TDB3s41PE3MhOFdoBI5+399B7DGeKaOi3QccQVstH+gYt4lVPc/uNCfUzsSwaejIa8mfmBh4FLgTpgq5ltdM69nsnzysD18kOfZ/qBivY6cjNGwFxHqLnWjvB0sdCLv0J2EVzVjzFga+tEJlff2T5iRKJhvsoW0tzayi1uNWPtAPvcSB7kWmbM+RJjYtMRKNS9I9NX8tOBN2PLAGJmq4GrAYX8INV1Xc8F54zjat9v2m/knUvnunJhP666DYhU/YRxjOzxwR2vcw4OuyKGWVNcmH8RgGs2zUw6sZh4R6ZDvgTYG/e+DjgnfgczWwQsAhg/fnyGuyPZFr8qEESXfNtd2LHk22+qP8wk3xvR0kEar7Z9rpVTaBiUV/BtKyy1YjwRmclKu4mjTa3HhblCfXDI+Y1X51w5UA4QCoXSUSyVHOm6ODPAbF8lrwR+zLCiY+1t8Uu+ne/b2fNCEv0Qwcd+G8kYGtJ/8DzR6jr+DmvFMBz73Cjub5nPxtYZ+Aw+c854ds2ZktN+Sm5lOuTrgXFx70tjbeIB8aWXRGNNZvsq+U7gBxRY8r+7MxHwzsFTkZl8ePqljGirycdtSzK4pPMxEuwT35bsdZv3KaKJAMW8TyPDKKSZYRwjkVasYyx7N+JH17zHcO51n2dd03kAnDQ0wLKrJjFnWgkPAQ/1eDQZLDId8luBM8zsdKLhfi3wmQyfUzKsoqaeOzZs5/2mjtEejmgp5nr/5k6Bl44QjzggFoStGEdcIcPtGBF8+GjlCEUEacKHI4KPJyMX80ZoGZ+bPYWtEJuPZT9vM5JvN8/n2dYZmR5AydCAj6KAn8YjzRQPDXCsOdJesurKZ9Gr8kTiwzsQ134y8K+xL5HumEu6RHqaTmB2BfAg0SGUjznnvpVs31Ao5KqqqjLaH+m9ipp67t64k8ZwMxANnCvLTuOZ6nrCzZ2XdFte8Bif82/uV6h3Xbw5/v+S71PE4yd9je/8ZRoR5/AZFBX4CDe34jcj4hxDAz7CLa04R/vN3HtUopBBxMyqnXOhhNsyHfJ9oZDPDxU19SxdX0s4yZXnbF8ltxWspcT2d2rvV8AD7448F3fgDx3jsz+ih25E+qK7kM/5jVfJH11HwiQy21fJysCjnR6k6av4Jd/s7M9zatzCERqfLZJeCnk5riwD0TC/N/AYw+iY5+R9htBEQUoBj/mws2/UikAiWaKQH+SipZnthJsjzPZVsqzgPzjZDgPHl1+Gc5R+V/e0lqdITijkB7lVm3a3B/yqwCPRlYC60a/RMgp4kZxRyA8SFTX1rNq0+7jH2Pc1hmPj2X9IgSWvxfeaAl0kryjkB4H4kgxAfWOYpeu3A3DD8Fe4rfnRvgV88GQoHBabtjemy+yPIpIfFPIe1HbVXt8Ybh9L3lW4OcKqTbt5IbCGoS19uJHq88Mnvq0VgEQGCIW8x3S9ak8U8G32NYYZOuSd3h+8cBh88kEFvMgAopD3kIqaer6+9rVugz3e2OIgFJV2Lrt0FTxZV+4iA5hC3iParuCTBXzbU6pjbT/73KjovOKzvgT+u+DZxdAc7tg5EISrHlKwi3iAQt4j2oZCJtL1KdVS289K/6MU+M/qCPItK+BQHYwohZl3KeBFPEIhP0Alekq1q/g5ZrqOby+IHI0Ge9n8ji8R8RyF/ABTUVPP8md38t6R7sO97cnVbh9eOlSX/g6KSF5RyA8gXUfOJNKnCcRGlKaxdyKSjxTyA0h3dXegb0+uBoLR2ruIeJpCfgCIf7gpmbYr+F4F/IhxurkqMkhkLOTN7G7gH6F9JeV/cc79Z6bO51W9KdEA3FawtucSjYZGigw6mb6Sf8A5p2Uo+6G3V+9tY997nB1SDzWJDEoq1+Sh3ly9Ly94jOsLNuPr6WDmh7k/VLiLDFI9ZkSKvmJmtWb2mJmdlGgHM1tkZlVmVtXQ0JBol0GnpwebXi1axOd6E/CBoAJeZJBLKeTNbLOZ7UjwdTXwA+DvganA28C/JTqGc67cORdyzoVGjx6dSnc8Y1+SEk3bzdWT7TDdV2csenNV9XeRQS+lco1z7pLe7GdmPwJ+kcq5BpOxxcFOtfjunlw9zohxcOuOzHZQRAaMjJVrzOy0uLdzASVPLy2ZNZFgwA90XL2X+noR8JjGvotIJ5m88Xq/mU0FHLAHuDmD5/KUOdNKKNn7C8a9uopTXUMv11U1CC1UeUZEOslYyDvnrs/UsT2vdi0f3b4MCNND8T1KwyNFJAkNocyyZAtqd7JlRef53ZPRk6si0gOFfBZV1NSz5Gev0dwaXdijvjHMkp+9BtA56HuaHVJPropIL2V6nLzEVNTUc+uabe0B36a51XH3xp2dd+5udkgNjRSRPlDIZ0HbE6yO6GiZysLFvFX0GSoLFzPbV3n8wh8z74perccLBGHej6LDIxXwItJLKtdkQdsTrLN9lawKPEKRRZ9mLbX9rAo8As0AV3b8gJbkE5E0Uchnwb7GMLN9lTwQ+D7+LqNliizC8sL/AO7rvEFL8olIGijks+CG4a9wW/OjxwV8m2IOZ7dDIjJoqCafBbcF1nQ713uvnnUSEekHhXwWDA2/0/0OwZOz0xERGXQU8tnQ3ZBIf2H0aVURkQxQyGdDoiGREL2Cv/ph3WAVkYzRjdds0JBIEckRhXy2aEikiOSAyjUiIh6mkO+t2rXwwGS4uzj6vXZtrnskItIjlWt6o3YtPLu4Y/rfQ3uj70ElGBHJa6ku5P1pM9tpZq1mFuqybamZvWlmu81sVmrdzLFE87s3h6PtIiJ5LNUr+R3APOCR+EYzOxO4FpgEjAU2m9kHnXORFM+XG8nmd+9p3ncRkRxL6UreObfLObc7waargdXOuWPOuT8CbwLTUzlXTiV7mKm7h5xERPJApm68lgB7497XxdoGpmTzu8+8Kzf9ERHppR5D3sw2m9mOBF9Xp6MDZrbIzKrMrKqhoSEdh0y/svlsnbKcdxhNqzPeYTRbpyzXTVcRyXs91uSdc5f047j1wLi496WxtkTHLwfKAUKhkEu0T9rUru3XU6cVNfUs3foBws3fbW8LbvVz37j64xfhFhHJI5kq12wErjWzIjM7HTgDeCVD5+qdtmGQh/YCrmMYZC/Gu7et7BQv3Bxh1aZEtyNERPJHqkMo55pZHfAx4Dkz2wTgnNsJrAVeB/4L+HLOR9akMAxyX2O4T+0iIvkipSGUzrkNwIYk274FfCuV46dVCsMgxxYHqU8Q6GOLE8wsKSKSR7wxrUFvphxIYRjkklkTCQb8ndqCAT9LZk3sT29FRLJm4Id8b2vtKQyDnDOthPvmTaGkOIgBJcVB7ps3RTddRSTvmXOZHdDSF6FQyFVVVfXthx6YHAv4LkaMg1t3dG7r5+gaEZF8ZmbVzrlQom0Df4KyvtTaNae7iAwyA79coykHRESSGvghrykHRESSGvghXzYfrnooWoPHot+vekhlGRERvFCTB9XaRUSSGPhX8iIikpRCXkTEwxTyIiIeppAXEfEwhbyIiIcp5EVEPEwhLyLiYQp5EREPS3VlqE+b2U4zazWzUFz7BDMLm9m22NcPU++qiIj0VapPvO4A5gGPJNj2B+fc1BSPLyIiKUh1+b9dAGaWnt5kWEVNPas27WZfY5ixxUGWzJqohT9ExNMyWZM/3cxqzOxXZnZ+Bs/TKxU19Sxdv536xjAOqG8Ms3T9dipq6nPdNRGRjOkx5M1ss5ntSPB1dTc/9jYw3jk3Dfgn4CkzOzHJ8ReZWZWZVTU0NPTvU/TCqk27CTdHOrWFmyOs2rQ7Y+cUEcm1Hss1zrlL+npQ59wx4FjsdbWZ/QH4IHDc2n7OuXKgHKLL//X1XL21rzHcp3YRES/ISLnGzEabmT/2+u+AM4C3MnGu3hpbHOxTu4iIF6Q6hHKumdUBHwOeM7NNsU0XALVmtg1YB3zROXcwta6mZsmsiQQD/k5twYCfJbMm5qhHIiKZl+romg3AhgTtzwDPpHLsdGsbRaPRNSIymHhjZahemjOtRKEuIoOKpjUQEfEwhbyIiIcp5EVEPEwhLyLiYQp5EREPU8iLiHiYQl5ExMMU8iIiHqaQFxHxMIW8iIiHeWJaA634JCKS2IAP+bYVn9oWBGlb8QlQ0IvIoDfgyzVa8UlEJLkBH/Ja8UlEJLkBH/Ja8UlEJLkBH/Ja8UlEJLlUl/9bZWa/N7NaM9tgZsVx25aa2ZtmttvMZqXe1cTmTCvhvnlTKCkOYkBJcZD75k3RTVcREcCcc/3/YbPLgF8651rM7NsAzrl/NrMzgaeB6cBYYDPwQedcJPnRIBQKuaqqqn73R0RkMDKzaudcKNG2lK7knXP/7Zxrib39LVAae301sNo5d8w590fgTaKBLyIiWZTOmvxC4PnY6xJgb9y2uljbccxskZlVmVlVQ0NDGrsjIiI9PgxlZpuBMQk23eGc+3lsnzuAFuDJvnbAOVcOlEO0XNPXnxcRkeR6DHnn3CXdbTezzwOfBGa6jgJ/PTAubrfSWJuIiGRRqqNrLgduA2Y7547EbdoIXGtmRWZ2OnAG8Eoq5xIRkb5LdXTNm0ARcCDW9Fvn3Bdj2+4gWqdvAW5xzj2f+CidjtcA/KnfHcpPo4D9ue5Ehnn9M3r984E+40D3Aefc6EQbUgp56ZmZVSUb2uQVXv+MXv98oM/oZQP+iVcREUlOIS8i4mEK+cwrz3UHssDrn9Hrnw/0GT1LNXkREQ/TlbyIiIcp5EVEPEwhnwXdTcnsBWb2aTPbaWatZuapIWpmdnlsuuw3zez2XPcn3czsMTN718x25LovmRd30OcAAAGSSURBVGBm48zsf8zs9dj/R7+W6z5lm0I+O14AJjvnyoD/ByzNcX/SbQcwD3gp1x1JJzPzAw8DnwDOBBbEptH2kp8Cl+e6ExnUAnzdOXcmcC7wZQ/+DrulkM+CbqZk9gTn3C7nnBdXTp8OvOmce8s51wSsJjqNtmc4514CDua6H5ninHvbOfdq7PXfgF0kmRHXqxTy2Rc/JbPkt15PmS35z8wmANOA3+W2J9nV4yyU0juZnpI513rz+UTylZkNB54hOo/WX3Pdn2xSyKdJP6dkHjB6+nwepSmzPcDMAkQD/knn3Ppc9yfbVK7Jgm6mZJb8thU4w8xON7NC4Fqi02jLAGFmBvwY2OWc+06u+5MLCvns+HfgBOAFM9tmZj/MdYfSyczmmlkd8DHgOTPblOs+pUPsZvlXgE1Eb9itdc7tzG2v0svMngZeBiaaWZ2ZfSHXfUqzjwPXAxfH/uxtM7Mrct2pbNK0BiIiHqYreRERD1PIi4h4mEJeRMTDFPIiIh6mkBcR8TCFvIiIhynkRUQ87P8DGluQJ7NUCtMAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "|trained our first neural network! 🥳🥳🥳\n",
        "\n",
        "In order to be able to build NN libs and layers such as nn.Linear (PyTorch syntax), etc. we need a couple more tricks up our sleeves! 🔥"
      ],
      "metadata": {
        "id": "7LGpm4DCyVMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom PyTrees ⚙️"
      ],
      "metadata": {
        "id": "J2MjyPqhypZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyContainer:  # this could be a linear layer a conv layer or whatever\n",
        "    \"\"\"A named container.\"\"\"\n",
        "    def __init__(self, name: str, a: int, b: int, c: int):\n",
        "        self.name = name\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.c = c"
      ],
      "metadata": {
        "id": "OIG6mcxNx_oK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_pytree = [MyContainer('Alice', 1, 2, 3), MyContainer('Bob', 4, 5, 6)]  # 8 leaves? Right? Noup.\n",
        "\n",
        "leaves = jax.tree_leaves(example_pytree)\n",
        "print(f\"{repr(example_pytree):<45}\\n has {len(leaves)} leaves:\\n {leaves}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa7YZYdmzQTE",
        "outputId": "2d23a821-539d-4d78-bcde-aff8e31a20c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<__main__.MyContainer object at 0x7fc987e4e6d0>, <__main__.MyContainer object at 0x7fc987e4e750>]\n",
            " has 2 leaves:\n",
            " [<__main__.MyContainer object at 0x7fc987e4e6d0>, <__main__.MyContainer object at 0x7fc987e4e750>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.tree_map(lambda x: x + 1, example_pytree))  # this will not work :/ it'd be nice if it did"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "bKFvoUX6zZc_",
        "outputId": "1ddde92f-2142-4f68-ccfe-207710e070d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-80b5eee956da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_pytree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# this will not work :/ it'd be nice if it did\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtree_multimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtree_multimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-80b5eee956da>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_pytree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# this will not work :/ it'd be nice if it did\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'MyContainer' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "tHlUa_WuzqLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get it to work! We'll need to define 2 functions (flatten/unflatten)\n",
        "\n",
        "def flatten_MyContainer(container):\n",
        "    \"\"\"Returns an iterable over container contents, and aux data.\"\"\"\n",
        "    flat_contents = [container.a, container.b, container.c]\n",
        "\n",
        "    # we don't want the name to appear as a child, so it is auxiliary data.\n",
        "    # auxiliary data is usually a description of the structure of a node,\n",
        "    # e.g., the keys of a dict -- anything that isn't a node's children.\n",
        "    aux_data = container.name\n",
        "\n",
        "    return flat_contents, aux_data\n",
        "\n",
        "def unflatten_MyContainer(aux_data, flat_contents):\n",
        "    \"\"\"Converts aux data and the flat contents into a MyContainer.\"\"\"\n",
        "    return MyContainer(aux_data, *flat_contents)\n",
        "\n",
        "# Register a custom PyTree node\n",
        "jax.tree_util.register_pytree_node(MyContainer, flatten_MyContainer, unflatten_MyContainer)"
      ],
      "metadata": {
        "id": "9EsIEkkgzkBp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "leaves = jax.tree_leaves(example_pytree)\n",
        "print(f\"{repr(example_pytree):<45}\\n has {len(leaves)} leaves:\\n {leaves}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EdDjZ41zzno",
        "outputId": "7134b61a-17f4-48e7-d5b1-1fd58c46df9f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<__main__.MyContainer object at 0x7fc987e4e6d0>, <__main__.MyContainer object at 0x7fc987e4e750>]\n",
            " has 6 leaves:\n",
            " [1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try applying tree map again\n",
        "result = jax.tree_map(lambda x: x + 1, example_pytree)\n",
        "print(jax.tree_leaves(result))  # it works now as expected!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQZlGCDCz4a7",
        "outputId": "d7c67310-e192-4b07-e099-22490e080ccf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 4, 5, 6, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mistake nodes as leaves"
      ],
      "metadata": {
        "id": "sM8XnX8w0Uw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally a common gotcha working with PyTrees: mistaking nodes for leaves/children\n",
        "\n",
        "zeros_tree = [jnp.zeros((2, 3)), jnp.zeros((3, 4))]\n",
        "print(zeros_tree)\n",
        "\n",
        "# Try to make another tree with ones instead of zeros\n",
        "shapes = jax.tree_map(lambda x: x.shape, zeros_tree)  # sol: simply add a jnp.array\n",
        "print(shapes)\n",
        "\n",
        "ones_tree = jax.tree_map(jnp.ones, shapes)\n",
        "print(ones_tree)\n",
        "# bad result for 1 but cirrect for 0\n",
        "# Task: debug this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK0_-zgzz7_O",
        "outputId": "a8358e76-0fea-45b4-bd87-9b05d356a677"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DeviceArray([[0., 0., 0.],\n",
            "             [0., 0., 0.]], dtype=float32), DeviceArray([[0., 0., 0., 0.],\n",
            "             [0., 0., 0., 0.],\n",
            "             [0., 0., 0., 0.]], dtype=float32)]\n",
            "[(2, 3), (3, 4)]\n",
            "[(DeviceArray([1., 1.], dtype=float32), DeviceArray([1., 1., 1.], dtype=float32)), (DeviceArray([1., 1., 1.], dtype=float32), DeviceArray([1., 1., 1., 1.], dtype=float32))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally a common gotcha working with PyTrees: mistaking nodes for leaves/children\n",
        "# CORRECT\n",
        "zeros_tree = [jnp.zeros((2, 3)), jnp.zeros((3, 4))]\n",
        "print(zeros_tree)\n",
        "\n",
        "# Try to make another tree with ones instead of zeros\n",
        "shapes = jax.tree_map(lambda x: jnp.array(x.shape), zeros_tree)  # sol: simply add a jnp.array\n",
        "print(shapes)\n",
        "\n",
        "ones_tree = jax.tree_map(jnp.ones, shapes)\n",
        "print(ones_tree)\n",
        "# bad result for 1 but cirrect for 0\n",
        "# Task: debug this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AjSXKFr0KNX",
        "outputId": "fc6550e6-9808-4c91-ef98-9f3ef5dc8314"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DeviceArray([[0., 0., 0.],\n",
            "             [0., 0., 0.]], dtype=float32), DeviceArray([[0., 0., 0., 0.],\n",
            "             [0., 0., 0., 0.],\n",
            "             [0., 0., 0., 0.]], dtype=float32)]\n",
            "[DeviceArray([2, 3], dtype=int32), DeviceArray([3, 4], dtype=int32)]\n",
            "[DeviceArray([[1., 1., 1.],\n",
            "             [1., 1., 1.]], dtype=float32), DeviceArray([[1., 1., 1., 1.],\n",
            "             [1., 1., 1., 1.],\n",
            "             [1., 1., 1., 1.]], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We can now create custom layers and we can train even bigger neural networks!\n",
        "\n",
        "But what if our neural network is really big? How do we handle training it across multiple devices?\n",
        "\n",
        "I'm glad you asked."
      ],
      "metadata": {
        "id": "B3_Gb1Bp04om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelism in JAX 💻💻\n",
        "Parallelism in JAX is handled by another fundamental transform function: pmap\n",
        "\n",
        "pmap basics"
      ],
      "metadata": {
        "id": "RELome2z20g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try to get ourselves some TPU goodness\n",
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "jax.devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT7Q6nIR0usQ",
        "outputId": "654145df-3999-4998-d28f-b8f6e05f8a49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use a simple running example here\n",
        "x = np.arange(5)  # signal\n",
        "w = np.array([2., 3., 4.])  # window/kernel\n",
        "\n",
        "def convolve(w, x):  # implementation of 1D convolution/correlation\n",
        "    output = []\n",
        "\n",
        "    for i in range(1, len(x)-1):\n",
        "        output.append(jnp.dot(x[i-1:i+2], w))\n",
        "\n",
        "    return jnp.array(output)\n",
        "\n",
        "result = convolve(w, x)\n",
        "print(x)\n",
        "# print(w) like convolution\n",
        "print(repr(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqUYoGdS24AS",
        "outputId": "5a74b702-28f5-4ab6-a183-4e0489dcc47b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4]\n",
            "[2. 3. 4.]\n",
            "DeviceArray([11., 20., 29.], dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IZjb14m68oI",
        "outputId": "620a98e1-7178-4a05-af78-78e19ac086a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([11., 20., 29.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = jax.local_device_count() \n",
        "print(f'Number of available devices: {n_devices}')\n",
        "\n",
        "# Let's now imagine we have a much heavier load (a batch of examples)\n",
        "xs = np.arange(5 * n_devices).reshape(-1, 5)\n",
        "ws = np.stack([w] * n_devices)\n",
        "print(xs)\n",
        "print(ws)\n",
        "print(xs.shape, ws.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXVZ_xy43vff",
        "outputId": "bbb4411a-da8f-424b-b394-f5d526d4eec8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of available devices: 8\n",
            "[[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]\n",
            " [15 16 17 18 19]\n",
            " [20 21 22 23 24]\n",
            " [25 26 27 28 29]\n",
            " [30 31 32 33 34]\n",
            " [35 36 37 38 39]]\n",
            "[[2. 3. 4.]\n",
            " [2. 3. 4.]\n",
            " [2. 3. 4.]\n",
            " [2. 3. 4.]\n",
            " [2. 3. 4.]\n",
            " [2. 3. 4.]\n",
            " [2. 3. 4.]\n",
            " [2. 3. 4.]]\n",
            "(8, 5) (8, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First way to optimize this is to simply use vmap\n",
        "vmap_result = jax.vmap(convolve)(ws, xs)\n",
        "print(repr(vmap_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Sxvfj3743Nu",
        "outputId": "f8e15978-56e4-462a-ae7d-5d93f30a075c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeviceArray([[ 11.,  20.,  29.],\n",
            "             [ 56.,  65.,  74.],\n",
            "             [101., 110., 119.],\n",
            "             [146., 155., 164.],\n",
            "             [191., 200., 209.],\n",
            "             [236., 245., 254.],\n",
            "             [281., 290., 299.],\n",
            "             [326., 335., 344.]], dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The amazing thing is if you just swap vmap for pmap you are now running on multiple\n",
        "# devices. How cool is that?\n",
        "pmap_result = jax.pmap(convolve)(ws, xs)\n",
        "print(repr(pmap_result))  # ShardedDeviceArray!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MliRJd95XbT",
        "outputId": "c8ce32bd-0aaf-4e9e-f952-08f3acd8cb89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ShardedDeviceArray([[ 11.,  20.,  29.],\n",
            "                    [ 56.,  65.,  74.],\n",
            "                    [101., 110., 119.],\n",
            "                    [146., 155., 164.],\n",
            "                    [191., 200., 209.],\n",
            "                    [236., 245., 254.],\n",
            "                    [281., 290., 299.],\n",
            "                    [326., 335., 344.]], dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# No cross-device communication costs. Computations are done independently on each dev.\n",
        "double_pmap_result = jax.pmap(convolve)(jax.pmap(convolve)(ws, xs), xs)\n",
        "print(repr(double_pmap_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ladcuABJ5Xvo",
        "outputId": "03adbcbd-c76e-4e34-e31f-08c1fa5f4577"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ShardedDeviceArray([[   78.,   138.,   198.],\n",
            "                    [ 1188.,  1383.,  1578.],\n",
            "                    [ 3648.,  3978.,  4308.],\n",
            "                    [ 7458.,  7923.,  8388.],\n",
            "                    [12618., 13218., 13818.],\n",
            "                    [19128., 19863., 20598.],\n",
            "                    [26988., 27858., 28728.],\n",
            "                    [36198., 37203., 38208.]], dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Same results but we don't have to manually broadcast w (recall: same as for vmap!)\n",
        "pmap_smarter_result = jax.pmap(convolve, in_axes=(None, 0))(w, xs)\n",
        "print(repr(pmap_smarter_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIZXxC1n5ovD",
        "outputId": "546fd6bf-cc88-4326-da3f-a112b071f666"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ShardedDeviceArray([[ 11.,  20.,  29.],\n",
            "                    [ 56.,  65.,  74.],\n",
            "                    [101., 110., 119.],\n",
            "                    [146., 155., 164.],\n",
            "                    [191., 200., 209.],\n",
            "                    [236., 245., 254.],\n",
            "                    [281., 290., 299.],\n",
            "                    [326., 335., 344.]], dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is all great, but oftentimes we do need to communicate between devices. 📱\n",
        "\n",
        "Let's see how that is handled."
      ],
      "metadata": {
        "id": "RCVhH9tq5sTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Communication between devices 📱📱📱📱"
      ],
      "metadata": {
        "id": "31QuUMQp5wOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Same example as above but this time we communicate across devices \n",
        "# in order to normalize the outputs\n",
        "def normalized_convolution(w, x):\n",
        "    output = []\n",
        "\n",
        "    for i in range(1, len(x)-1):\n",
        "        output.append(jnp.dot(x[i-1:i+2], w))\n",
        "\n",
        "    output = jnp.array(output)  # same result as before\n",
        "\n",
        "    return output / jax.lax.psum(output, axis_name='batch_dim')  # this is where communication happens\n",
        "    # summ of on axis batch dim, then normalize\n",
        "res_pmap = jax.pmap(normalized_convolution, axis_name='batch_dim', in_axes=(None, 0))(w, xs)\n",
        "res_vmap = jax.vmap(normalized_convolution, axis_name='batch_dim', in_axes=(None, 0))(w, xs)\n",
        "\n",
        "# refer 46 min in vid\n",
        "\n",
        "print(repr(res_pmap))\n",
        "print(repr(res_vmap))\n",
        "\n",
        "print(f'Verify the output is normalized: {sum(res_pmap[:, 0])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD6O2r8k5pJ9",
        "outputId": "b563cf86-7fdc-4697-92c1-25be80819ce5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ShardedDeviceArray([[0.00816024, 0.01408451, 0.019437  ],\n",
            "                    [0.04154303, 0.04577465, 0.04959785],\n",
            "                    [0.07492582, 0.07746479, 0.07975871],\n",
            "                    [0.10830861, 0.10915492, 0.10991956],\n",
            "                    [0.14169139, 0.14084506, 0.14008042],\n",
            "                    [0.17507419, 0.17253521, 0.17024128],\n",
            "                    [0.20845698, 0.20422535, 0.20040214],\n",
            "                    [0.24183977, 0.23591548, 0.23056298]], dtype=float32)\n",
            "DeviceArray([[0.00816024, 0.01408451, 0.019437  ],\n",
            "             [0.04154303, 0.04577465, 0.04959785],\n",
            "             [0.07492582, 0.07746479, 0.07975871],\n",
            "             [0.10830861, 0.10915492, 0.10991956],\n",
            "             [0.14169139, 0.14084506, 0.14008042],\n",
            "             [0.17507419, 0.17253521, 0.17024128],\n",
            "             [0.20845698, 0.20422535, 0.20040214],\n",
            "             [0.24183977, 0.23591548, 0.23056298]], dtype=float32)\n",
            "Verify the output is normalized: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "couple more usefull func\n",
        "- value_and_grad\n",
        "-"
      ],
      "metadata": {
        "id": "7Cxr12136Zok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes aside from grads we also need to return the loss value (for logging, etc.)\n",
        "\n",
        "def sum_squared_error(x, y):\n",
        "    return sum((x-y)**2)\n",
        "\n",
        "x = jnp.arange(4, dtype=jnp.float32)\n",
        "y = x + 0.1\n",
        "print(x,y)\n",
        "# An efficient way to return both grads and loss value\n",
        "jax.value_and_grad(sum_squared_error)(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHqClVt66bM5",
        "outputId": "c2dd07ff-7906-4248-c16e-b98c21250592"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2. 3.] [0.1 1.1 2.1 3.1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(0.03999997, dtype=float32),\n",
              " DeviceArray([-0.2       , -0.20000005, -0.19999981, -0.19999981], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And sometimes the loss function needs to return intermediate results\n",
        " \n",
        "def sum_squared_error_with_aux(x, y):\n",
        "    return sum((x-y)**2), x-y\n",
        "\n",
        "# jax.grad(sum_squared_error_with_aux)(x, y)  # has_aux=True\n",
        "# grad only for scalar func\n",
        "\n",
        "# for intermediate values\n",
        "\n",
        "jax.grad(sum_squared_error_with_aux,has_aux=True)(x, y)  # has_aux=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXg2_wjK6nHk",
        "outputId": "b7a9732f-ab34-4245-d5e8-d13c6dd4c7d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([-0.2       , -0.20000005, -0.19999981, -0.19999981], dtype=float32),\n",
              " DeviceArray([-0.1       , -0.10000002, -0.0999999 , -0.0999999 ], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a very simple model in parallel!"
      ],
      "metadata": {
        "id": "hFbDGnj-9tcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Params(NamedTuple):\n",
        "    weight: jnp.ndarray\n",
        "    bias: jnp.ndarray\n",
        "\n",
        "\n",
        "lr = 0.005\n",
        "\n",
        "\n",
        "def init_model(rng):\n",
        "    weights_key, bias_key = jax.random.split(rng)\n",
        "    weight = jax.random.normal(weights_key, ())\n",
        "    bias = jax.random.normal(bias_key, ())\n",
        "    return Params(weight, bias)\n",
        "\n",
        "\n",
        "def forward(params, xs):\n",
        "    return params.weight * xs + params.bias\n",
        "\n",
        "\n",
        "def loss_fn(params, xs, ys):\n",
        "    pred = forward(params, xs)\n",
        "    return jnp.mean((pred - ys) ** 2)  # MSE\n",
        "\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='batch')\n",
        "def update(params, xs, ys):\n",
        "\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, xs, ys)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
        "\n",
        "    # Each device performs its own SGD update, but since we start with the same params\n",
        "    # and synchronise gradients, the params stay in sync on each device.\n",
        "    new_params = jax.tree_multimap(\n",
        "        lambda param, g: param - g * lr, params, grads)\n",
        "    \n",
        "    # If we were using Adam or another stateful optimizer,\n",
        "    # we would also do something like:\n",
        "    # updates, new_optimizer_state = optimizer(grads, optimizer_state)\n",
        "    # and then use updates instead of grads to actually update the params.\n",
        "    # (And we'd include the new_optimizer_state in the output, naturally.)\n",
        "\n",
        "    return new_params, loss"
      ],
      "metadata": {
        "id": "PHpxlFIO6qjT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate true data from y = w*x + b + noise\n",
        "true_w, true_b = 2, -1\n",
        "xs = np.random.normal(size=(128, 1))\n",
        "noise = 0.5 * np.random.normal(size=(128, 1))\n",
        "# noise = 0\n",
        "ys = xs * true_w + true_b + noise\n",
        "\n",
        "plt.scatter(xs, ys)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "eIgwopjT9yiK",
        "outputId": "b8bb9a2f-3366-4a0c-c1e8-f8769e74246e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise parameters and replicate across devices.\n",
        "params = init_model(jax.random.PRNGKey(0))\n",
        "n_devices = jax.local_device_count()\n",
        "replicated_params = jax.tree_map(lambda x: jnp.array([x] * n_devices), params)\n",
        "print(replicated_params)\n",
        "\n",
        "# Prepare the data\n",
        "def reshape_for_pmap(data, n_devices):\n",
        "    return data.reshape(n_devices, data.shape[0] // n_devices, *data.shape[1:])\n",
        "\n",
        "x_parallel = reshape_for_pmap(xs, n_devices)\n",
        "y_parallel = reshape_for_pmap(ys, n_devices)\n",
        "\n",
        "print(x_parallel.shape, y_parallel.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYD5-UKF91sm",
        "outputId": "9d5b1fa6-f8c0-47c6-9d88-8abfd8bf6eb0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params(weight=DeviceArray([0.14389051, 0.14389051, 0.14389051, 0.14389051, 0.14389051,\n",
            "             0.14389051, 0.14389051, 0.14389051], dtype=float32), bias=DeviceArray([-1.2515285, -1.2515285, -1.2515285, -1.2515285, -1.2515285,\n",
            "             -1.2515285, -1.2515285, -1.2515285], dtype=float32))\n",
            "(8, 16, 1) (8, 16, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "53WFU6bX-Apt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## continued in the orginal notebook"
      ],
      "metadata": {
        "id": "rFR-y1By-no4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N9jvetun-pbp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}